{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##TSWA Practical 6 - Text Preprocessing and Normalization"
      ],
      "metadata": {
        "id": "Ze_uak4_F7Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "NGandzC7G5dP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXcrjbIkXe92",
        "outputId": "c5ea5351-34c0-4371-9823-e016d1810256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b': -4em;\\r\\n    margin-left: 4em;\\r\\n    margin-top: 1em;\\r\\n    margin-bottom: 0;\\r\\n    font-size: medium\\r\\n}\\r\\n#pg-header #pg-header-authlist {\\r\\n    all: initial;\\r\\n    margin-top: 0;\\r\\n    margin-bottom: 0;\\r\\n}\\r\\n#pg-header #pg-machine-header strong {\\r\\n    font-weight: normal;\\r\\n}\\r\\n#pg-header #pg-start-separator, #pg-footer #pg-end-separator {\\r\\n    margin-bottom: 3em;\\r\\n    margin-left: 0;\\r\\n    margin-right: auto;\\r\\n    margin-top: 2em;\\r\\n    text-align: center\\r\\n}\\r\\n\\r\\n    .xhtml_center {text-align: center; display: block;}\\r\\n    .xhtml_center table {\\r\\n        display: table;\\r\\n        text-align: left;\\r\\n        margin-left: auto;\\r\\n        margin-right: auto;\\r\\n        }</style><title>The Project Gutenberg eBook of The Bible, King James version, Book 1: Genesis, by Anonymous</title><style>/* ************************************************************************\\r\\n * classless css copied from https://www.pgdp.net/wiki/CSS_Cookbook/Styles\\r\\n * ********************************************************************** */\\r\\n/* ************************************************************************\\r\\n * set the body margins to allow whitespace along sides of window\\r\\n * ******************************************'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.content\n",
        "print(content[1000:2200])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-Removing HTML Tags. Import re"
      ],
      "metadata": {
        "id": "wv8777DAHBK4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "txZz7Ov3Xp93"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbkwZChhXr-D"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFVLcAi0XvDh"
      },
      "outputs": [],
      "source": [
        "def html_stripping(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "  return stripped_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZzku6XSX13l"
      },
      "outputs": [],
      "source": [
        "clean_content = html_stripping(content)\n",
        "print(clean_content[1036:2045])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYS-xvoJX4V6"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from pprint import pprint\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nEE2rkFYE80"
      },
      "outputs": [],
      "source": [
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9qtzlaKfKDK"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "import re\n",
        "import string\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM2r-aFzUo0x"
      },
      "outputs": [],
      "source": [
        "# Creating our own corpus\n",
        "corpus=[\"The brown fox wasn't quick and he couldn't win the race.\",\n",
        "\"Hey it was a great cricket match @yesterday!!\",\n",
        "\"I just bought a @new mobile for me at $1000.\",\n",
        "\"Python NLP is really ****amazing*****!!@@@\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN-datxmpgeB"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "  sentences=nltk.sent_tokenize(text)\n",
        "  print(sentences)\n",
        "  word_tokens=[nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmNp78CMpl-T"
      },
      "outputs": [],
      "source": [
        "# Get the token list from the above def\n",
        "token_list=[tokenize_text(text) for text in corpus]\n",
        "print(token_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 - Removing special characters"
      ],
      "metadata": {
        "id": "EYbxtX41HFto"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2clNxxTZA7d"
      },
      "outputs": [],
      "source": [
        "def remove_special_characters(sentence,keep_apostrophes=False):\n",
        "  sentence = sentence.strip()\n",
        "  if keep_apostrophes:\n",
        "    PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
        "    filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
        "  else:\n",
        "    PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
        "    filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
        "  return filtered_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9D5mNkhr3LN"
      },
      "outputs": [],
      "source": [
        "sentence=\"The brown fox wasn't quick and he couldn't win the race.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmqq49HQoM-a"
      },
      "outputs": [],
      "source": [
        "remove_special_characters(sentence,keep_apostrophes=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwJprYHEZlNu"
      },
      "outputs": [],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmr81PCkZp8z"
      },
      "outputs": [],
      "source": [
        "import contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJNuqzySZtWj"
      },
      "outputs": [],
      "source": [
        "contraction_mapping=contractions.contractions_dict\n",
        "print(contraction_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-Replacing contractions with expanded form"
      ],
      "metadata": {
        "id": "uuje7uGxHKbz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua7EsEXlZzq0"
      },
      "outputs": [],
      "source": [
        "def contraction_expansion(sentence,contraction_mapping):\n",
        "  contractions_pattern=re.compile('({})'.format('|'.join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8XZSoyhcFcx"
      },
      "outputs": [],
      "source": [
        "# To remove the accented characters\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_1fdW76cIXi"
      },
      "outputs": [],
      "source": [
        "# def for removing accented characters\n",
        "def accented_char_removal(text):\n",
        "  text=str(text)\n",
        "  text = unicodedata.normalize('NFKD',text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKcTwbxqglKm"
      },
      "outputs": [],
      "source": [
        "accented_char_removal('Sómě Áccěntěd těxt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4 - Special Character Removal"
      ],
      "metadata": {
        "id": "IayMdwo_HQgX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNdqndd4cVca"
      },
      "outputs": [],
      "source": [
        "def special_char_removal(text, remove_digits=False):\n",
        "  pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "  text = re.sub(pattern, '', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54-bsBQ4csCt"
      },
      "outputs": [],
      "source": [
        "special_char_removal(\"Well this was fun! What do you think? 123#@!\",remove_digits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9at4lqedDh_"
      },
      "outputs": [],
      "source": [
        "# 4_ Stemming\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtHzyHZ3dE_p"
      },
      "outputs": [],
      "source": [
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUvJvrZFdHn3"
      },
      "outputs": [],
      "source": [
        "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5- Stemming"
      ],
      "metadata": {
        "id": "ZkXfbkKsHTwi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppRPojcadL96"
      },
      "outputs": [],
      "source": [
        "def simple_stemmer(text):\n",
        "  ps = nltk.porter.PorterStemmer()\n",
        "  text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRBFzanZdZW5"
      },
      "outputs": [],
      "source": [
        "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6 - Lemmetization"
      ],
      "metadata": {
        "id": "phH-56_DHY22"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_iQtRo9dflc"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M96vY_JodmMc"
      },
      "outputs": [],
      "source": [
        "wnl = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY8WdKJ7dufO"
      },
      "outputs": [],
      "source": [
        "# lemmatize nouns with 'n'\n",
        "print(wnl.lemmatize('cars', 'n')) # s is removed\n",
        "print(wnl.lemmatize('men', 'n'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF_2TuZ9dzjp"
      },
      "outputs": [],
      "source": [
        "# lemmatize verbs with 'v'\n",
        "print(wnl.lemmatize('running', 'v')) # extracting the verb from the verb phrase - running\n",
        "print(wnl.lemmatize('ate', 'v')) # verb behind ate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE_Jt2mWd6Ol"
      },
      "outputs": [],
      "source": [
        "# lemmatize adjectives with 'a'\n",
        "print(wnl.lemmatize('saddest', 'a')) # orginal adjective extracted\n",
        "print(wnl.lemmatize('fancier', 'a'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baxyrdfceFaK"
      },
      "outputs": [],
      "source": [
        "# 6 - def for lemmatization\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU0s-o1wenRG"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLmuD1Qbe_Ly"
      },
      "outputs": [],
      "source": [
        "# 5 - def for lemmatization\n",
        "def text_lemmatization(text):\n",
        "  text = nlp(text)\n",
        "  text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-2VvlfqfM3n"
      },
      "outputs": [],
      "source": [
        "text_lemmatization('My system keeps crashing! his crashed yesterday, ours crashes daily')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7 - Stopwords removal"
      ],
      "metadata": {
        "id": "O1twRh78HmPz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0IHQcqWj6uI"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EQN7MOsj8VJ"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ec_oisFkBf1"
      },
      "outputs": [],
      "source": [
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "print(stopword_list[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93rqei12kF7n"
      },
      "outputs": [],
      "source": [
        "# 7_ def remove stop words\n",
        "def stopword_removal(tokens):\n",
        "  stopword_list = nltk.corpus.stopwords.words('english')\n",
        "  filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "  return filtered_tokens\n",
        "  if is_lower_case:\n",
        "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "  else:\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "  filtered_text =''.join(filtered_tokens)\n",
        "  return filtered_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3Xil_aDkPIR"
      },
      "outputs": [],
      "source": [
        "stopword_removal(\"The, and, if are stopwords, computer is not\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2Wt8W8ETacI"
      },
      "outputs": [],
      "source": [
        "corpus =(\"US unveils world's most powerful supercomputer, beats China. \"\n",
        "\"The US has unveiled the world's most powerful supercomputer called 'Summit', \"\n",
        "\"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
        "\"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
        "\"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
        "\"which reportedly take up the size of two tennis courts.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMTCnKS_T2G-"
      },
      "outputs": [],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKYkBbFUrbrs"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyyNw0ESpXem"
      },
      "outputs": [],
      "source": [
        "def normalize_corpus(corpus,html_stripping=True,contraction_expansion=True,accented_char_removal=True,text_lower_case=True,\n",
        "                     text_lemmatization=True,special_char_removal=True,stopword_removal=True):\n",
        "  normalized_corpus = []\n",
        "  # normalize each document in the corpus\n",
        "  for doc in corpus:\n",
        "    # strip HTML\n",
        "    if html_stripping:\n",
        "      doc = html_stripping(doc)\n",
        "    if contraction_expansion:\n",
        "      doc = contraction_expansion(doc)\n",
        "    if accented_char_removal:\n",
        "      doc = accented_char_removal(doc)\n",
        "    if text_lower_case:\n",
        "      doc = doc.lower()\n",
        "      # remove extra newlines\n",
        "      doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "    # lemmatize text\n",
        "    if text_lemmatization:\n",
        "      doc = text_lemmatization(doc)\n",
        "    # remove special characters and\\or digits\n",
        "    if special_char_removal:\n",
        "      # insert spaces between special characters to isolate them\n",
        "      special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "      doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "      doc = special_char_removal(doc, remove_digits=True)\n",
        "      # remove extra whitespace\n",
        "      doc = re.sub(' +', ' ', doc)\n",
        "    # remove stopwords\n",
        "    if stopword_removal:\n",
        "      doc = stopword_removal(doc)\n",
        "      normalized_corpus.append(doc)\n",
        "  return normalized_corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndNskOIap0Td"
      },
      "outputs": [],
      "source": [
        "normalize_corpus(corpus,html_stripping=html_stripping,contraction_expansion=TextBlob,accented_char_removal=accented_char_removal,text_lower_case=True,text_lemmatization=text_lemmatization,special_char_removal=special_char_removal,stopword_removal=stopword_removal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw6f_ZI_uhVv"
      },
      "outputs": [],
      "source": [
        "def normalize_corpus(corpus):\n",
        "  normalized_corpus = []\n",
        "  for text in corpus:\n",
        "    # 1\n",
        "    text=html_stripping(text)\n",
        "    # 2\n",
        "    text=special_char_removal(text)\n",
        "    # 3\n",
        "    text=contraction_expansion(text,contraction_mapping)\n",
        "    # 4\n",
        "    # text=simple_stemmer(text)\n",
        "    # 5\n",
        "    text=accented_char_removal(text)\n",
        "    # 6\n",
        "    text=text_lemmatization(text)\n",
        "    # 7\n",
        "    text=stopword_removal(text)\n",
        "    normalized_corpus.append(text)\n",
        "    #if tokenize:\n",
        "    #  text = tokenize_text(text)\n",
        "    # normalized_corpus.append(text)\n",
        "  return normalized_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doclljwKxbBx"
      },
      "outputs": [],
      "source": [
        "normalize_corpus(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quVqkLbir17c"
      },
      "outputs": [],
      "source": [
        "# Install Text Normalizer for extracting the newsgoup data set\n",
        "!pip install text_normalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPt2BLsNlQbf"
      },
      "outputs": [],
      "source": [
        "import text_normalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPUWf7fDMWhL"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfu510qpMgJn"
      },
      "outputs": [],
      "source": [
        "# Create objects for each package\n",
        "import numpy as np\n",
        "#import text_normalizer as tn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Wait for 60 seconds before trying again\n",
        "time.sleep(60)\n",
        "\n",
        "# Fetch data\n",
        "data = fetch_20newsgroups(subset='all')\n",
        "data = fetch_20newsgroups(subset='all', shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
        "data_labels_map = dict(enumerate(data.target_names))\n",
        "\n",
        "print(\"Dataset fetched successfully!\")"
      ],
      "metadata": {
        "id": "O1v_S5jlNiT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-82brBxgsL9"
      },
      "outputs": [],
      "source": [
        "# building the dataframe for the data extracted from newgroups\n",
        "# Create a corpus of newsgroup sentences\n",
        "corpus=data.data\n",
        "target_labels=data.target\n",
        "target_names = [data_labels_map[label] for label in data.target]\n",
        "data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels,'Target Name': target_names})\n",
        "print(data_df.shape)\n",
        "data_df.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfMDTvZbkH1f"
      },
      "source": [
        "From this dataset, we can see that each document has some textual content and the\n",
        "label can be denoted by a specific number, which maps to a newsgroup category name"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}